{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-12T13:27:58.629681Z","iopub.status.busy":"2024-08-12T13:27:58.629304Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.3)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n"]}],"source":["!pip install -q translate\n","!pip install -q underthesea==1.3.5a3\n","!pip install -q underthesea[deep]\n","!pip install -q pyvi\n","!pip install -q langdetect\n","!pip install -q googletrans==3.1.0a0\n","!pip install -q peft\n","!pip install bitsandbytes\n","!pip install transformers\n","!pip install flash-attn\n","!pip install -U sentence-transformers\n","!pip install xformers\n","!pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip install git+https://github.com/JaidedAI/EasyOCR.git"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/duong/anaconda3/envs/aic/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/duong/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e55a7d4324f65581af5f483e830b80f34680e8ff/modeling_hf_nomic_bert.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = loader(resolved_archive_file)\n","<All keys matched successfully>\n"]}],"source":["from transformers import DPRContextEncoder, AutoProcessor, DPRContextEncoderTokenizer, BlipModel,TrOCRProcessor, VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, BitsAndBytesConfig, BlipForConditionalGeneration\n","#from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n","from sentence_transformers import SentenceTransformer\n","\n","\n","# tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', use_fast=False)\n","# embedding_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n","\n","embedding_model= SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import clip\n","from PIL import Image\n","import faiss\n","import numpy as np\n","import json\n","import matplotlib.pyplot as plt\n","import math\n","import googletrans\n","import translate\n","import glob\n","import underthesea\n","import sys\n","import time\n","from tqdm import tqdm\n","from pyvi import ViUtils, ViTokenizer\n","from difflib import SequenceMatcher\n","from langdetect import detect\n","from pathlib import Path\n","import re\n","\n","import easyocr"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using CPU. Note: This module is much faster with a GPU.\n","/home/duong/anaconda3/envs/aic/lib/python3.8/site-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/home/duong/anaconda3/envs/aic/lib/python3.8/site-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(model_path, map_location=device)\n"]}],"source":["reader = easyocr.Reader(['vi','en'], gpu=False)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/duong/Documents/AIC2024_UTE_AI_Unknown/src\n","/home/duong/Documents/AIC2024_UTE_AI_Unknown/src/notebook\n","/home/duong/Documents/AIC2024_UTE_AI_Unknown\n","/home/duong/Documents/AIC2024_UTE_AI_Unknown/data/keyframes\n"]}],"source":["ROOT = Path(os.getcwd()).resolve()\n","\n","# Add ROOT to sys.path\n","sys.path.append(str(ROOT))\n","\n","# Determine the working directory\n","if len(ROOT.parents) > 1:\n","    WORK_DIR = ROOT.parents[0]\n","else:\n","    WORK_DIR = ROOT  # Fallback to ROOT if it doesn't have enough parents\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","\n","print(WORK_DIR)\n","print(ROOT)\n","\n","if len(WORK_DIR.parents) >1:\n","    Path_root = WORK_DIR.parents[0]\n","else:\n","    Path_root = WORK_DIR\n","\n","paths = f\"{WORK_DIR.parents[0]}/data/keyframes\"\n","des_path =  f\"{WORK_DIR}/working/dicts/npy_ocr\"\n","\n","paths_root = f\"{Path_root}/data/keyframes\"\n","os.makedirs(des_path, exist_ok=True)\n","print(Path_root)\n","print(paths_root)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Text_Preprocessing():\n","    def __init__(self, stopwords_path=f\"{Path_root}/data/vietnamese-stopwords-dash.txt\"):\n","        with open(stopwords_path, 'r', encoding='utf-8') as f:  # Open in text mode for easier string handling\n","            self.stop_words = [line.strip() for line in f.readlines()]\n","\n","    def find_substring(self, string1, string2):\n","        match = SequenceMatcher(None, string1, string2, autojunk=False).find_longest_match(0, len(string1), 0, len(string2))\n","        return string1[match.a:match.a + match.size].strip()\n","\n","    def remove_stopwords(self, text):\n","        text = ViTokenizer.tokenize(text)\n","        filtered_words = [w for w in text.split() if w not in self.stop_words]\n","        return \" \".join(filtered_words)\n","\n","    def lowercasing(self, text):\n","        return text.lower()\n","\n","    def uppercasing(self, text):\n","        return text.upper()\n","\n","    def add_accents(self, text):\n","        return ViUtils.add_accents(text)\n","\n","    def remove_accents(self, text):\n","        return ViUtils.remove_accents(text)\n","\n","    def sentence_segment(self, text):\n","        return underthesea.sent_tokenize(text)\n","\n","    def text_norm(self, text):\n","        return underthesea.text_normalize(text)\n","\n","    def text_classify(self, text):\n","        return underthesea.classify(text)\n","\n","    def sentiment_analysis(self, text):\n","        return underthesea.sentiment(text)\n","\n","    def __call__(self, text):\n","        # Apply preprocessing steps\n","        text = self.lowercasing(text)\n","        # text = self.remove_stopwords(text)\n","        # Uncomment and adjust as needed\n","        # text = self.remove_accents(text)\n","        # text = self.add_accents(text)\n","        text = self.text_norm(text)\n","        return text  # Return the processed text\n","\n","class Translation():\n","    def __init__(self, from_lang='vi', to_lang='en', mode='google'):\n","        # The class Translation is a wrapper for the two translation libraries, googletrans and translate.\n","        self.__mode = mode\n","        self.__from_lang = from_lang\n","        self.__to_lang = to_lang\n","        self.text_processing = Text_Preprocessing()\n","        if mode in 'googletrans':\n","            self.translator = googletrans.Translator()\n","        elif mode in 'translate':\n","            self.translator = translate.Translator(from_lang=from_lang,to_lang=to_lang)\n","\n","    def preprocessing(self, text):\n","\n","        return self.text_processing(text) #text.lower()\n","\n","    def __call__(self, text):\n","\n","        text = self.preprocessing(text)\n","        return self.translator.translate(text) if self.__mode in 'translate' \\\n","                else self.translator.translate(text, dest=self.__to_lang).text\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["reader = easyocr.Reader(['vi','en'])\n","# Function to perform OCR on an image and return text\n","def ocr_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n","    generated_ids = model.generate(pixel_values)\n","    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    return generated_text\n","\n","def easyocr_image(image_path):\n","    texts = []\n","    result = reader.readtext(image_path)\n","\n","    for i in result:\n","        if i[2] >0.3:\n","          texts.append(i[1])\n","\n","    merged_text = \"\\n\".join(texts)\n","    # merged_text = Translation(merged_text)\n","    return merged_text\n","\n","# def easyocr_image(image_path):\n","#     texts = []\n","#     result = reader.readtext(image_path)\n","#     for i in result:\n","#         if i[2] > 0.3:\n","#             texts.append(i[1])\n","\n","#     if texts:\n","#         merged_text = \"\\n\".join(texts)\n","#         return merged_text\n","#     else:\n","#         return \"\"\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 235/235 [03:18<00:00,  1.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed /home/duong/Documents/AIC2024_UTE_AI_Unknown/data/keyframes/Keyframes_L30/L30_V001 in 198.32217693328857 seconds\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 273/273 [03:52<00:00,  1.18it/s]\n"," 50%|█████     | 1/2 [07:10<07:10, 430.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Processed /home/duong/Documents/AIC2024_UTE_AI_Unknown/data/keyframes/Keyframes_L30/L30_V002 in 430.43214678764343 seconds\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 241/241 [03:34<00:00,  1.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed /home/duong/Documents/AIC2024_UTE_AI_Unknown/data/keyframes/Keyframes_L31/L31_V001 in 214.08043456077576 seconds\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 244/244 [03:36<00:00,  1.12it/s]\n","100%|██████████| 2/2 [14:21<00:00, 430.71s/it]"]},{"name":"stdout","output_type":"stream","text":["Processed /home/duong/Documents/AIC2024_UTE_AI_Unknown/data/keyframes/Keyframes_L31/L31_V002 in 430.99586963653564 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["Translate = Translation()\n","for keyframe in tqdm(os.listdir(paths_root)):\n","  path_keyframe = os.path.join(paths_root,keyframe)\n","  video_paths = sorted(glob.glob(f\"{path_keyframe}/*/\"))\n","  video_paths = ['/'.join(i.split('/')[:-1]) for i in video_paths]\n","\n","  start_time = time.time()\n","  for vd_path in video_paths:\n","\n","    re_feats = []\n","    keyframe_paths = glob.glob(f'{vd_path}/*.jpg')\n","    keyframe_paths = sorted(keyframe_paths, key=lambda x : x.split('/')[-1].replace('.jpg',''))\n","\n","    for keyframe_path in tqdm(keyframe_paths):\n","\n","\n","      #text = ocr_image(keyframe_path)\n","\n","      #//////////////////////////////////\n","      text = easyocr_image(keyframe_path)\n","      #//////////////////////////////////\n","      # if detect(text) == 'vi' :\n","      #   text = Translation(text)\n","      # if detect(text) == 'vi':\n","      text = Translate(text)\n","\n","      # Convert text to embedding vector\n","      #embedding = embedding_model(**tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True,max_length=512, add_special_tokens = True)).pooler_output.detach().numpy()\n","\n","      embeddings = embedding_model.encode(text)\n","\n","      # Append embedding to re_feats list\n","      re_feats.append(embeddings)\n","\n","    name_npy = vd_path.split('/')[-1]\n","\n","    # Construct output file path\n","    outfile = os.path.join(des_path, f'{name_npy}.npy')\n","\n","    # Ensure the directory exists before saving\n","    os.makedirs(des_path, exist_ok=True)\n","    np.save(outfile, re_feats)\n","\n","    print(f\"Processed {vd_path} in {time.time() - start_time} seconds\")\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded /home/duong/Documents/AIC2024_UTE_AI_Unknown/src/working/dicts/npy_ocr/L30_V001.npy, shape: (235, 768)\n","Loaded /home/duong/Documents/AIC2024_UTE_AI_Unknown/src/working/dicts/npy_ocr/L30_V002.npy, shape: (273, 768)\n","Loaded /home/duong/Documents/AIC2024_UTE_AI_Unknown/src/working/dicts/npy_ocr/L31_V001.npy, shape: (241, 768)\n","Loaded /home/duong/Documents/AIC2024_UTE_AI_Unknown/src/working/dicts/npy_ocr/L31_V002.npy, shape: (244, 768)\n","Saved /home/duong/Documents/AIC2024_UTE_AI_Unknown/src/working/dicts/bin_ocr/faiss_OCR_cosine.bin\n"]}],"source":["feature_shape = 384\n","\n","\n","def write_bin_file_ocr(bin_path: str, npy_path: str, method='cosine'):\n","    if method in 'L2':\n","      index = faiss.IndexFlatL2(feature_shape)\n","    elif method in 'cosine':\n","      index = faiss.IndexFlatIP(feature_shape)\n","    else:\n","      assert f\"{method} not supported\"\n","    npy_files = glob.glob(os.path.join(npy_path, \"*.npy\"))\n","    npy_files_sorted = sorted(npy_files)\n","\n","    for npy_file in npy_files_sorted:\n","        feats = np.load(npy_file)\n","        print(f\"Loaded {npy_file}, shape: {feats.shape}\")\n","\n","\n","        # Convert to float32 and reshape to match feature_shape\n","        feats = feats.astype(np.float32)\n","        feats = feats.reshape(-1, feats.shape[-1])\n","\n","        # Resize or trim feats_normalized to match feature_shape if necessary\n","        if feats.shape[1] != feature_shape:\n","            feats = feats[:, :feature_shape]\n","\n","        assert feats.shape[1] == feature_shape, \\\n","            f\"Query features dimension {feats.shape[1]} do not match index dimension {feature_shape}\"\n","\n","        # Add to Faiss index\n","        index.add(feats)\n","\n","    # Write the Faiss index to disk\n","    faiss.write_index(index, os.path.join(bin_path, f\"faiss_OCR_{method}.bin\"))\n","    print(f'Saved {os.path.join(bin_path, f\"faiss_OCR_{method}.bin\")}')\n","\n","\n","# write ocr\n","write_bin_file_ocr(bin_path=f\"{WORK_DIR}/working/dicts/bin_ocr\", npy_path=f\"{WORK_DIR}/working/dicts/npy_ocr\")\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":4}
