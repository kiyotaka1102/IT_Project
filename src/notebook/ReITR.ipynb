{"cells":[{"cell_type":"markdown","metadata":{"id":"McZM9W-idDUq"},"source":["# Installation\n","Install RelTR and import necessary packages."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-12T08:52:28.739622Z","iopub.status.busy":"2024-08-12T08:52:28.739036Z","iopub.status.idle":"2024-08-12T08:52:35.851647Z","shell.execute_reply":"2024-08-12T08:52:35.850625Z","shell.execute_reply.started":"2024-08-12T08:52:28.739591Z"},"executionInfo":{"elapsed":25110,"status":"ok","timestamp":1722785602224,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"m2TH79Ufb5Fp","outputId":"f1992e06-388a-415e-a293-b1110ddbc7a2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'RelTR'...\n","remote: Enumerating objects: 317, done.\u001b[K\n","remote: Counting objects: 100% (317/317), done.\u001b[K\n","remote: Compressing objects: 100% (175/175), done.\u001b[K\n","remote: Total 317 (delta 140), reused 282 (delta 120), pack-reused 0\u001b[K\n","Receiving objects: 100% (317/317), 27.49 MiB | 55.09 MiB/s, done.\n","Resolving deltas: 100% (140/140), done.\n","/kaggle/working/RelTR\n"]}],"source":["!git clone https://github.com/yrcong/RelTR.git\n","%cd RelTR/\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"VmHBU0_LdqWH"},"source":["# VG labels\n","VG 150 enitiy classes and 50 relationship classes."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:52:55.222185Z","iopub.status.busy":"2024-08-12T08:52:55.221495Z","iopub.status.idle":"2024-08-12T08:52:55.233520Z","shell.execute_reply":"2024-08-12T08:52:55.232512Z","shell.execute_reply.started":"2024-08-12T08:52:55.222150Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1722785602227,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"aZfsXdDzdiPG","trusted":true},"outputs":[],"source":["CLASSES = [ 'N/A', 'airplane', 'animal', 'arm', 'bag', 'banana', 'basket', 'beach', 'bear', 'bed', 'bench', 'bike',\n","                'bird', 'board', 'boat', 'book', 'boot', 'bottle', 'bowl', 'box', 'boy', 'branch', 'building',\n","                'bus', 'cabinet', 'cap', 'car', 'cat', 'chair', 'child', 'clock', 'coat', 'counter', 'cow', 'cup',\n","                'curtain', 'desk', 'dog', 'door', 'drawer', 'ear', 'elephant', 'engine', 'eye', 'face', 'fence',\n","                'finger', 'flag', 'flower', 'food', 'fork', 'fruit', 'giraffe', 'girl', 'glass', 'glove', 'guy',\n","                'hair', 'hand', 'handle', 'hat', 'head', 'helmet', 'hill', 'horse', 'house', 'jacket', 'jean',\n","                'kid', 'kite', 'lady', 'lamp', 'laptop', 'leaf', 'leg', 'letter', 'light', 'logo', 'man', 'men',\n","                'motorcycle', 'mountain', 'mouth', 'neck', 'nose', 'number', 'orange', 'pant', 'paper', 'paw',\n","                'people', 'person', 'phone', 'pillow', 'pizza', 'plane', 'plant', 'plate', 'player', 'pole', 'post',\n","                'pot', 'racket', 'railing', 'rock', 'roof', 'room', 'screen', 'seat', 'sheep', 'shelf', 'shirt',\n","                'shoe', 'short', 'sidewalk', 'sign', 'sink', 'skateboard', 'ski', 'skier', 'sneaker', 'snow',\n","                'sock', 'stand', 'street', 'surfboard', 'table', 'tail', 'tie', 'tile', 'tire', 'toilet', 'towel',\n","                'tower', 'track', 'train', 'tree', 'truck', 'trunk', 'umbrella', 'vase', 'vegetable', 'vehicle',\n","                'wave', 'wheel', 'window', 'windshield', 'wing', 'wire', 'woman', 'zebra']\n","\n","REL_CLASSES = ['__background__', 'above', 'across', 'against', 'along', 'and', 'at', 'attached to', 'behind',\n","                'belonging to', 'between', 'carrying', 'covered in', 'covering', 'eating', 'flying in', 'for',\n","                'from', 'growing on', 'hanging from', 'has', 'holding', 'in', 'in front of', 'laying on',\n","                'looking at', 'lying on', 'made of', 'mounted on', 'near', 'of', 'on', 'on back of', 'over',\n","                'painted on', 'parked on', 'part of', 'playing', 'riding', 'says', 'sitting on', 'standing on',\n","                'to', 'under', 'using', 'walking in', 'walking on', 'watching', 'wearing', 'wears', 'with']\n"]},{"cell_type":"markdown","metadata":{"id":"qNb3xdkZeKy6"},"source":["# Build and load the pretrained model"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"collapsed":true,"execution":{"iopub.execute_input":"2024-08-12T08:54:38.141099Z","iopub.status.busy":"2024-08-12T08:54:38.140385Z","iopub.status.idle":"2024-08-12T08:54:40.254322Z","shell.execute_reply":"2024-08-12T08:54:40.253441Z","shell.execute_reply.started":"2024-08-12T08:54:38.141066Z"},"executionInfo":{"elapsed":13313,"status":"error","timestamp":1722785615534,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"TeWdzd5LeOGQ","jupyter":{"outputs_hidden":true},"outputId":"971237dd-9ab2-4c1c-decc-77f1d67547be","trusted":true},"outputs":[{"data":{"text/plain":["RelTR(\n","  (transformer): Transformer(\n","    (encoder): TransformerEncoder(\n","      (layers): ModuleList(\n","        (0-5): 6 x TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.1, inplace=False)\n","          (dropout2): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (decoder): TransformerDecoder(\n","      (layers): ModuleList(\n","        (0-5): 6 x TransformerDecoderLayer(\n","          (self_attn_entity): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout2_entity): Dropout(p=0.1, inplace=False)\n","          (norm2_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_attn_entity): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout1_entity): Dropout(p=0.1, inplace=False)\n","          (norm1_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (self_attn_so): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout2_so): Dropout(p=0.1, inplace=False)\n","          (norm2_so): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_attn_sub): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout1_sub): Dropout(p=0.1, inplace=False)\n","          (norm1_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_sub_entity): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout2_sub): Dropout(p=0.1, inplace=False)\n","          (norm2_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_attn_obj): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout1_obj): Dropout(p=0.1, inplace=False)\n","          (norm1_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (cross_obj_entity): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (dropout2_obj): Dropout(p=0.1, inplace=False)\n","          (norm2_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (linear1_entity): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout3_entity): Dropout(p=0.1, inplace=False)\n","          (linear2_entity): Linear(in_features=2048, out_features=256, bias=True)\n","          (dropout4_entity): Dropout(p=0.1, inplace=False)\n","          (norm3_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (linear1_sub): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout3_sub): Dropout(p=0.1, inplace=False)\n","          (linear2_sub): Linear(in_features=2048, out_features=256, bias=True)\n","          (dropout4_sub): Dropout(p=0.1, inplace=False)\n","          (norm3_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (linear1_obj): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout3_obj): Dropout(p=0.1, inplace=False)\n","          (linear2_obj): Linear(in_features=2048, out_features=256, bias=True)\n","          (dropout4_obj): Dropout(p=0.1, inplace=False)\n","          (norm3_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","  (backbone): Joiner(\n","    (0): Backbone(\n","      (body): IntermediateLayerGetter(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): FrozenBatchNorm2d()\n","        (relu): ReLU(inplace=True)\n","        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","        (layer1): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): FrozenBatchNorm2d()\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer2): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d()\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (3): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer3): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d()\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (3): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (4): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (5): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer4): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d()\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d()\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d()\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d()\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (1): PositionEmbeddingSine()\n","  )\n","  (entity_embed): Embedding(100, 512)\n","  (triplet_embed): Embedding(200, 768)\n","  (so_embed): Embedding(2, 256)\n","  (entity_class_embed): Linear(in_features=256, out_features=152, bias=True)\n","  (entity_bbox_embed): MLP(\n","    (layers): ModuleList(\n","      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n","      (2): Linear(in_features=256, out_features=4, bias=True)\n","    )\n","  )\n","  (so_mask_conv): Sequential(\n","    (0): Upsample(size=(28, 28), mode='nearest')\n","    (1): Conv2d(2, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n","    (2): ReLU(inplace=True)\n","    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (so_mask_fc): Sequential(\n","    (0): Linear(in_features=2048, out_features=512, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=512, out_features=128, bias=True)\n","  )\n","  (rel_class_embed): MLP(\n","    (layers): ModuleList(\n","      (0): Linear(in_features=640, out_features=256, bias=True)\n","      (1): Linear(in_features=256, out_features=52, bias=True)\n","    )\n","  )\n","  (sub_class_embed): Linear(in_features=256, out_features=152, bias=True)\n","  (sub_bbox_embed): MLP(\n","    (layers): ModuleList(\n","      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n","      (2): Linear(in_features=256, out_features=4, bias=True)\n","    )\n","  )\n","  (obj_class_embed): Linear(in_features=256, out_features=152, bias=True)\n","  (obj_bbox_embed): MLP(\n","    (layers): ModuleList(\n","      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n","      (2): Linear(in_features=256, out_features=4, bias=True)\n","    )\n","  )\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from models.backbone import Backbone, Joiner\n","from models.position_encoding import PositionEmbeddingSine\n","from models.transformer import Transformer\n","from models.reltr import RelTR\n","\n","position_embedding = PositionEmbeddingSine(128, normalize=True)\n","backbone = Backbone('resnet50', False, False, False)\n","backbone = Joiner(backbone, position_embedding)\n","backbone.num_channels = 2048\n","\n","transformer = Transformer(d_model=256, dropout=0.1, nhead=8,\n","                          dim_feedforward=2048,\n","                          num_encoder_layers=6,\n","                          num_decoder_layers=6,\n","                          normalize_before=False,\n","                          return_intermediate_dec=True)\n","\n","model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n","              num_entities=100, num_triplets=200)\n","\n","# The checkpoint is pretrained on Visual Genome\n","# ckpt = torch.hub.load_state_dict_from_url(\n","#     url='https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth',\n","#     map_location='cpu', check_hash=True)\n","ckpt = torch.load(\"/kaggle/input/reitr/other/default/1/checkpoint0149.pth\") #,map_location='cpu')\n","model.load_state_dict(ckpt['model'])\n","model.eval()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:54:47.115731Z","iopub.status.busy":"2024-08-12T08:54:47.115342Z","iopub.status.idle":"2024-08-12T08:54:47.123292Z","shell.execute_reply":"2024-08-12T08:54:47.122241Z","shell.execute_reply.started":"2024-08-12T08:54:47.115701Z"},"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1722785615535,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"58v9rajd1ONO","trusted":true},"outputs":[],"source":["# Some transformation functions\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","# for output bounding box post-processing\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","def rescale_bboxes(out_bbox, size):\n","    img_w, img_h = size\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","    return b"]},{"cell_type":"markdown","metadata":{"id":"4nGtLQrmAfJB"},"source":["# Load Image\n","You can replace the link with other images. Note that the entities in the used image should be included in the VG labels."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["im = Image.open('/kaggle/input/data-hcm-ai-challenge-2023-batch-1/Data/Keyframes_L01/L01_V001/000470.jpg')\n","plt.imshow(im)\n","plt.axis('off')  # Hide axes\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1722785615535,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"010vjjIsFjCF","trusted":true},"outputs":[],"source":["#url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Yellow_Happy.jpg/1200px-Yellow_Happy.jpg'\n","\n","# Apply the transformations\n","img = transform(im).unsqueeze(0)\n","\n","# Print the shape of the transformed image tensor\n","print(img.shape)"]},{"cell_type":"markdown","metadata":{"id":"Rx2Wez2qBdsE"},"source":["# Inference"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:59:21.693550Z","iopub.status.busy":"2024-08-12T08:59:21.692852Z","iopub.status.idle":"2024-08-12T09:00:46.880777Z","shell.execute_reply":"2024-08-12T09:00:46.879195Z","shell.execute_reply.started":"2024-08-12T08:59:21.693504Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  2%|▏         | 20/1203 [01:24<1:23:35,  4.24s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m output_file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, output_file_name)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(img_path, model, output_file_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m hooks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m.\u001b[39mbackbone[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, output: conv_features\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m ]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Process features and attention weights\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[1;32m     63\u001b[0m     hook\u001b[38;5;241m.\u001b[39mremove()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/RelTR/models/reltr.py:90\u001b[0m, in \u001b[0;36mRelTR.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, (\u001b[38;5;28mlist\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[1;32m     89\u001b[0m     samples \u001b[38;5;241m=\u001b[39m nested_tensor_from_tensor_list(samples)\n\u001b[0;32m---> 90\u001b[0m features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m src, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecompose()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/RelTR/models/backbone.py:100\u001b[0m, in \u001b[0;36mJoiner.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_list: NestedTensor):\n\u001b[0;32m--> 100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     out: List[NestedTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    102\u001b[0m     pos \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/RelTR/models/backbone.py:72\u001b[0m, in \u001b[0;36mBackboneBase.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_list: NestedTensor):\n\u001b[0;32m---> 72\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     out: Dict[\u001b[38;5;28mstr\u001b[39m, NestedTensor] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, x \u001b[38;5;129;01min\u001b[39;00m xs\u001b[38;5;241m.\u001b[39mitems():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import glob\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","# Define the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Assuming other necessary imports and initializations are here\n","\n","# Paths\n","input_images_path = \"/kaggle/input/data-hcm-ai-challenge-2023-batch-1/Data/Keyframes_L01/L01_V003\"  # Directory containing input images\n","output_directory = \"/kaggle/working/output_file_test2\"  # Directory to save output text files\n","\n","# Ensure the output directory exists\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# Function to process a single image\n","def process_image(img_path, model):\n","    im = Image.open(img_path).convert(\"RGB\")\n","    img = transform(im).unsqueeze(0)#.to(device)  # Adjust based on your model's input requirements\n","    texts = []\n","    with torch.no_grad():\n","        # propagate through the model\n","        \n","        outputs = model(img)\n","\n","        # keep only predictions with >0.3 confidence\n","        probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n","        probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n","        probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n","        keep = torch.logical_and(probas.max(-1).values > 0.3, \n","                                 torch.logical_and(probas_sub.max(-1).values > 0.3,\n","                                                   probas_obj.max(-1).values > 0.3))\n","\n","        # convert boxes from [0; 1] to image scales\n","        sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n","        obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n","\n","        topk = 10  # display up to 10 images\n","        keep_queries = torch.nonzero(keep, as_tuple=True)[0]\n","        indices = torch.argsort(-probas[keep_queries].max(-1)[0] * \n","                                probas_sub[keep_queries].max(-1)[0] * \n","                                probas_obj[keep_queries].max(-1)[0])[:topk]\n","        keep_queries = keep_queries[indices]\n","\n","        # save the attention weights\n","        conv_features, dec_attn_weights_sub, dec_attn_weights_obj = [], [], []\n","        hooks = [\n","            model.backbone[-2].register_forward_hook(\n","                lambda self, input, output: conv_features.append(output)\n","            ),\n","            model.transformer.decoder.layers[-1].cross_attn_sub.register_forward_hook(\n","                lambda self, input, output: dec_attn_weights_sub.append(output[1])\n","            ),\n","            model.transformer.decoder.layers[-1].cross_attn_obj.register_forward_hook(\n","                lambda self, input, output: dec_attn_weights_obj.append(output[1])\n","            )\n","        ]\n","\n","        # Process features and attention weights\n","        outputs = model(img)\n","        for hook in hooks:\n","            hook.remove()\n","\n","        conv_features = conv_features[0]\n","        dec_attn_weights_sub = dec_attn_weights_sub[0]\n","        dec_attn_weights_obj = dec_attn_weights_obj[0]\n","\n","        # get the feature map shape\n","        h, w = conv_features['0'].tensors.shape[-2:]\n","\n","        # Open the file in write mode\n","        #with open(output_file_path, 'w') as file:\n","        for idx, (sxmin, symin, sxmax, symax), (oxmin, oymin, oxmax, oymax) in \\\n","                zip(keep_queries, sub_bboxes_scaled[indices], obj_bboxes_scaled[indices]):\n","\n","            texts.append(title_text = (CLASSES[probas_sub[idx].argmax()] + ' ' +\n","                          REL_CLASSES[probas[idx].argmax()] + ' ' +\n","                          CLASSES[probas_obj[idx].argmax()]))\n","\n","            # Write the text to the file\n","\n","            #file.write(f\"Title: {title_text}\\n\\n\")\n","\n","                # Print the text to the console\n","#                 print(f\"Image: {img_path}\")\n","#                 print(f\"Title: {title_text}\\n\")\n","    merged_text = \"\\n\".join(texts)\n","    return merged_text\n","        \n","        \n","for keyframe in tqdm(os.listdir(paths)):\n","  path_keyframe = os.path.join(paths,keyframe)\n","  video_paths = sorted(glob.glob(f\"{path_keyframe}/*/\"))\n","  video_paths = ['/'.join(i.split('/')[:-1]) for i in video_paths]\n","\n","  start_time = time.time()\n","  for vd_path in video_paths:\n","\n","    re_feats = []\n","    keyframe_paths = glob.glob(f'{vd_path}/*.jpg')\n","    keyframe_paths = sorted(keyframe_paths, key=lambda x : x.split('/')[-1].replace('.jpg',''))\n","\n","    for keyframe_path in tqdm(keyframe_paths):\n","\n","\n","      #text = ocr_image(keyframe_path)\n","\n","      #//////////////////////////////////\n","      text = process_image(keyframe_path, model)\n","      #//////////////////////////////////\n","      #if detect(text) == 'vi' :\n","      text = Translation(text)\n","\n","      # Convert text to embedding vector\n","      #embedding = embedding_model(**tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True,max_length=512, add_special_tokens = True)).pooler_output.detach().numpy()\n","      embeddings = embedding_model.encode(text)\n","      # Append embedding to re_feats list\n","      re_feats.append(embeddings)\n","\n","    name_npy = vd_path.split('/')[-1]\n","\n","    # Construct output file path\n","    outfile = os.path.join(des_path, f'{name_npy}.npy')\n","\n","    # Ensure the directory exists before saving\n","    os.makedirs(des_path, exist_ok=True)\n","    np.save(outfile, re_feats)\n","\n","    print(f\"Processed {vd_path} in {time.time() - start_time} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_shape = 512\n","\n","\n","def write_bin_file_ocr(bin_path: str, npy_path: str, method='cosine'):\n","    if method in 'L2':\n","      index = faiss.IndexFlatL2(feature_shape)\n","    elif method in 'cosine':\n","      index = faiss.IndexFlatIP(feature_shape)\n","    else:\n","      assert f\"{method} not supported\"\n","    npy_files = glob.glob(os.path.join(npy_path, \"*.npy\"))\n","    npy_files_sorted = sorted(npy_files)\n","\n","    for npy_file in npy_files_sorted:\n","        feats = np.load(npy_file)\n","        print(f\"Loaded {npy_file}, shape: {feats.shape}\")\n","\n","\n","        # Convert to float32 and reshape to match feature_shape\n","        feats = feats.astype(np.float32)\n","        feats = feats.reshape(-1, feats.shape[-1])\n","\n","        # Resize or trim feats_normalized to match feature_shape if necessary\n","        if feats.shape[1] != feature_shape:\n","            feats = feats[:, :feature_shape]\n","\n","        assert feats.shape[1] == feature_shape, \\\n","            f\"Query features dimension {feats.shape[1]} do not match index dimension {feature_shape}\"\n","\n","        # Add to Faiss index\n","        index.add(feats)\n","\n","    # Write the Faiss index to disk\n","    faiss.write_index(index, os.path.join(bin_path, f\"faiss_ReITR_{method}.bin\"))\n","    print(f'Saved {os.path.join(bin_path, f\"faiss_ReITR_{method}.bin\")}')\n","\n","\n","# write ocr\n","write_bin_file_ocr(bin_path=f\"{WORK_DIR}/data/dicts/bin_ReITR\", npy_path=f\"{WORK_DIR}/data/dicts/npy_ReITR\")\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1l8LXOOlp4x2vMDneiBN_3DhMWaB3MRXG","timestamp":1722785446571},{"file_id":"1-U642OoCyb8OSM8nx9lme49dmWa_aUcU","timestamp":1722785366171}],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3705417,"sourceId":6423178,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":102145,"modelInstanceId":77512,"sourceId":92445,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
