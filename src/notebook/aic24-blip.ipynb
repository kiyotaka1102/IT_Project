{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install -q translate\n","!pip install -q underthesea==1.3.5a3\n","!pip install -q underthesea[deep]\n","!pip install -q pyvi\n","!pip install -q langdetect\n","!pip install -q googletrans==3.1.0a0\n","!pip install -q peft\n","!pip install bitsandbytes\n","!pip install transformers\n","!pip install flash-attn\n","!pip install -U sentence-transformers\n","!pip install xformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q faiss-cpu\n","!pip install -q git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import clip\n","from PIL import Image\n","import faiss\n","import numpy as np\n","import json\n","import matplotlib.pyplot as plt\n","import math\n","import googletrans\n","import translate\n","import glob\n","import underthesea\n","import sys\n","import time\n","from tqdm import tqdm\n","from pyvi import ViUtils, ViTokenizer\n","from difflib import SequenceMatcher\n","from langdetect import detect\n","from pathlib import Path\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ROOT = Path(os.getcwd()).resolve()\n","\n","# Add ROOT to sys.path\n","sys.path.append(str(ROOT))\n","\n","# Determine the working directory\n","if len(ROOT.parents) > 1:\n","    WORK_DIR = ROOT.parents[0]\n","else:\n","    WORK_DIR = ROOT  # Fallback to ROOT if it doesn't have enough parents\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","paths = f\"{WORK_DIR}/data/keyframes\"\n","des_path =  f\"{WORK_DIR}/working//dicts/npy_blip\"\n","os.makedirs(des_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import DPRContextEncoder, AutoProcessor, DPRContextEncoderTokenizer, BlipModel,TrOCRProcessor, VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, BitsAndBytesConfig, BlipForConditionalGeneration\n","#from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n","from sentence_transformers import SentenceTransformer\n","\n","\n","# tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', use_fast=False)\n","# embedding_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n","\n","embedding_model= SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def blip_image(image_path):\n","  image = Image.open(image_path)\n","  inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","  pixel_values = inputs.pixel_values\n","\n","  generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n","  generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","  return generated_caption"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for keyframe in tqdm(os.listdir(paths)):\n","  path_keyframe = os.path.join(paths,keyframe)\n","  video_paths = sorted(glob.glob(f\"{path_keyframe}/*/\"))\n","  video_paths = ['/'.join(i.split('/')[:-1]) for i in video_paths]\n","\n","  start_time = time.time()\n","  for vd_path in video_paths:\n","\n","    re_feats = []\n","    keyframe_paths = glob.glob(f'{vd_path}/*.jpg')\n","    keyframe_paths = sorted(keyframe_paths, key=lambda x : x.split('/')[-1].replace('.jpg',''))\n","\n","    for keyframe_path in tqdm(keyframe_paths):\n","\n","\n","      #text = ocr_image(keyframe_path)\n","\n","      #//////////////////////////////////\n","      text = blip_image(keyframe_path)\n","      #//////////////////////////////////\n","      if detect(text) == 'vi' :\n","        text = Translation(text)\n","\n","      # Convert text to embedding vector\n","      #embedding = embedding_model(**tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True,max_length=512, add_special_tokens = True)).pooler_output.detach().numpy()\n","      embeddings = embedding_model.encode(text)\n","      # Append embedding to re_feats list\n","      re_feats.append(embeddings)\n","\n","    name_npy = vd_path.split('/')[-1]\n","\n","    # Construct output file path\n","    outfile = os.path.join(des_path, f'{name_npy}.npy')\n","\n","    # Ensure the directory exists before saving\n","    os.makedirs(des_path, exist_ok=True)\n","    np.save(outfile, re_feats)\n","\n","    print(f\"Processed {vd_path} in {time.time() - start_time} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_shape = 512\n","\n","\n","def write_bin_file_ocr(bin_path: str, npy_path: str, method='cosine'):\n","    if method in 'L2':\n","      index = faiss.IndexFlatL2(feature_shape)\n","    elif method in 'cosine':\n","      index = faiss.IndexFlatIP(feature_shape)\n","    else:\n","      assert f\"{method} not supported\"\n","    npy_files = glob.glob(os.path.join(npy_path, \"*.npy\"))\n","    npy_files_sorted = sorted(npy_files)\n","\n","    for npy_file in npy_files_sorted:\n","        feats = np.load(npy_file)\n","        print(f\"Loaded {npy_file}, shape: {feats.shape}\")\n","\n","\n","        # Convert to float32 and reshape to match feature_shape\n","        feats = feats.astype(np.float32)\n","        feats = feats.reshape(-1, feats.shape[-1])\n","\n","        # Resize or trim feats_normalized to match feature_shape if necessary\n","        if feats.shape[1] != feature_shape:\n","            feats = feats[:, :feature_shape]\n","\n","        assert feats.shape[1] == feature_shape, \\\n","            f\"Query features dimension {feats.shape[1]} do not match index dimension {feature_shape}\"\n","\n","        # Add to Faiss index\n","        index.add(feats)\n","\n","    # Write the Faiss index to disk\n","    faiss.write_index(index, os.path.join(bin_path, f\"faiss_BLIP_{method}.bin\"))\n","    print(f'Saved {os.path.join(bin_path, f\"faiss_BLIP_{method}.bin\")}')\n","\n","\n","# write ocr\n","write_bin_file_ocr(bin_path=f\"{WORK_DIR}/data/dicts/bin_blip\", npy_path=f\"{WORK_DIR}/data/dicts/npy_blip\")\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
